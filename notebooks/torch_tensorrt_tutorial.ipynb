{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f5ae353",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to import quantization op. Please install modelopt library (https://github.com/NVIDIA/TensorRT-Model-Optimizer?tab=readme-ov-file#installation) to add support for compiling quantized models\n",
      "TensorRT-LLM is not installed. Please install TensorRT-LLM or set TRTLLM_PLUGINS_PATH to the directory containing libnvinfer_plugin_tensorrt_llm.so to use converters for torch.distributed ops\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/21/2025-14:56:39] [TRT] [W] Functionality provided through tensorrt.plugin module is experimental.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_tensorrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84a96f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We begin by defining a model\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor):\n",
    "        x_out = self.relu(x)\n",
    "        y_out = self.relu(y)\n",
    "        x_y_out = x_out + y_out\n",
    "        return torch.mean(x_y_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74e90771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sample float inputs and initialize model\n",
    "sample_inputs = [torch.rand((5, 7)).cuda(), torch.rand((5, 7)).cuda()]\n",
    "model = Model().eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c76a7d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torch_tensorrt.dynamo._compiler:4 supported operations detected in subgraph containing 4 computational nodes. Skipping this subgraph, since min_block_size was detected to be 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.9144, device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next, we compile the model using torch.compile\n",
    "# For the default settings, we can simply call torch.compile\n",
    "# with the backend \"torch_tensorrt\", and run the model on an\n",
    "# input to cause compilation, as so:\n",
    "optimized_model = torch.compile(model, backend=\"torch_tensorrt\", dynamic=False)\n",
    "optimized_model(*sample_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96c9da8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we use Torch utilities to clean up the workspace\n",
    "# after the previous compile invocation\n",
    "torch._dynamo.reset()\n",
    "\n",
    "# Define sample half inputs and initialize model\n",
    "sample_inputs_half = [\n",
    "    torch.rand((5, 7)).half().cuda(),\n",
    "    torch.rand((5, 7)).half().cuda(),\n",
    "]\n",
    "model_half = Model().eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95cce0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch_tensorrt.dynamo.utils:Using Default Torch-TRT Runtime (as requested by user)\n",
      "INFO:torch_tensorrt.dynamo.utils:Device not specified, using Torch default current device - cuda:0. If this is incorrect, please specify an input device, via the device keyword.\n",
      "INFO:torch_tensorrt.dynamo.utils:Compilation Settings: CompilationSettings(enabled_precisions={<dtype.f16: 6>}, debug=True, workspace_size=0, min_block_size=2, torch_executed_ops={'torch.ops.aten.sub.Tensor'}, pass_through_build_failures=False, max_aux_streams=None, version_compatible=False, optimization_level=4, use_python_runtime=False, truncate_double=False, use_fast_partitioner=True, enable_experimental_decompositions=False, device=Device(type=DeviceType.GPU, gpu_id=0), require_full_compilation=False, disable_tf32=False, assume_dynamic_shape_support=False, sparse_weights=False, engine_capability=<EngineCapability.STANDARD: 1>, num_avg_timing_iters=1, dla_sram_size=1048576, dla_local_dram_size=1073741824, dla_global_dram_size=536870912, dryrun=False, hardware_compatible=False, timing_cache_path='/tmp/torch_tensorrt_engine_cache/timing_cache.bin', lazy_engine_init=False, cache_built_engines=False, reuse_cached_engines=False, use_explicit_typing=False, use_fp32_acc=False, refit_identical_engine_weights=False, strip_engine_weights=False, immutable_weights=True, enable_weight_streaming=False, enable_cross_compile_for_windows=False, use_aot_joint_export=True)\n",
      "\n",
      "DEBUG:torch_tensorrt.dynamo.backend.backends:Pre-AOT Autograd graph:\n",
      "graph():\n",
      "    %l_x_ : torch.Tensor [num_users=1] = placeholder[target=L_x_]\n",
      "    %l_y_ : torch.Tensor [num_users=1] = placeholder[target=L_y_]\n",
      "    %x_out : [num_users=1] = call_function[target=torch.nn.functional.relu](args = (%l_x_,), kwargs = {inplace: False})\n",
      "    %y_out : [num_users=1] = call_function[target=torch.nn.functional.relu](args = (%l_y_,), kwargs = {inplace: False})\n",
      "    %x_y_out : [num_users=1] = call_function[target=operator.add](args = (%x_out, %y_out), kwargs = {})\n",
      "    %mean : [num_users=1] = call_function[target=torch.mean](args = (%x_y_out,), kwargs = {})\n",
      "    return (mean,)\n",
      "DEBUG:torch_tensorrt.dynamo.lowering.passes.repair_input_aliasing:Inserted auxiliary clone nodes for placeholders:\n",
      "graph():\n",
      "    %l_x_ : torch.Tensor [num_users=1] = placeholder[target=L_x_]\n",
      "    %l_y_ : torch.Tensor [num_users=1] = placeholder[target=L_y_]\n",
      "    %clone_default_1 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%l_y_,), kwargs = {})\n",
      "    %clone_default : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%l_x_,), kwargs = {})\n",
      "    %x_out : [num_users=1] = call_function[target=torch.nn.functional.relu](args = (%clone_default,), kwargs = {inplace: False})\n",
      "    %y_out : [num_users=1] = call_function[target=torch.nn.functional.relu](args = (%clone_default_1,), kwargs = {inplace: False})\n",
      "    %x_y_out : [num_users=1] = call_function[target=operator.add](args = (%x_out, %y_out), kwargs = {})\n",
      "    %mean : [num_users=1] = call_function[target=torch.mean](args = (%x_y_out,), kwargs = {})\n",
      "    return (mean,)\n",
      "DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_sym_nodes:Removed SymInt placeholders:\n",
      "graph():\n",
      "    %l_x_ : torch.Tensor [num_users=1] = placeholder[target=L_x_]\n",
      "    %l_y_ : torch.Tensor [num_users=1] = placeholder[target=L_y_]\n",
      "    %clone_default_1 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%l_y_,), kwargs = {})\n",
      "    %clone_default : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%l_x_,), kwargs = {})\n",
      "    %x_out : [num_users=1] = call_function[target=torch.nn.functional.relu](args = (%clone_default,), kwargs = {inplace: False})\n",
      "    %y_out : [num_users=1] = call_function[target=torch.nn.functional.relu](args = (%clone_default_1,), kwargs = {inplace: False})\n",
      "    %x_y_out : [num_users=1] = call_function[target=operator.add](args = (%x_out, %y_out), kwargs = {})\n",
      "    %mean : [num_users=1] = call_function[target=torch.mean](args = (%x_y_out,), kwargs = {})\n",
      "    return (mean,)\n",
      "DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_detach:Removed 0 detach nodes:\n",
      "graph():\n",
      "    %l_x_ : torch.Tensor [num_users=1] = placeholder[target=L_x_]\n",
      "    %l_y_ : torch.Tensor [num_users=1] = placeholder[target=L_y_]\n",
      "    %clone_default_1 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%l_y_,), kwargs = {})\n",
      "    %clone_default : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%l_x_,), kwargs = {})\n",
      "    %x_out : [num_users=1] = call_function[target=torch.nn.functional.relu](args = (%clone_default,), kwargs = {inplace: False})\n",
      "    %y_out : [num_users=1] = call_function[target=torch.nn.functional.relu](args = (%clone_default_1,), kwargs = {inplace: False})\n",
      "    %x_y_out : [num_users=1] = call_function[target=operator.add](args = (%x_out, %y_out), kwargs = {})\n",
      "    %mean : [num_users=1] = call_function[target=torch.mean](args = (%x_y_out,), kwargs = {})\n",
      "    return (mean,)\n",
      "DEBUG:torch_tensorrt.dynamo.backend.backends:Post-AOT Autograd graph:\n",
      "graph():\n",
      "    %arg0_1 : [num_users=1] = placeholder[target=arg0_1]\n",
      "    %arg1_1 : [num_users=1] = placeholder[target=arg1_1]\n",
      "    %clone : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%arg1_1,), kwargs = {})\n",
      "    %clone_1 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%arg0_1,), kwargs = {})\n",
      "    %relu : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%clone_1,), kwargs = {})\n",
      "    %relu_1 : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%clone,), kwargs = {})\n",
      "    %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%relu, %relu_1), kwargs = {})\n",
      "    %mean : [num_users=1] = call_function[target=torch.ops.aten.mean.default](args = (%add,), kwargs = {})\n",
      "    return (mean,)\n",
      "DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_input_alias_fixing_clones:Removing node clone_1 from graph, since it is a clone node which is the only user of placeholder arg0_1 and was inserted by the compiler.\n",
      "DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_input_alias_fixing_clones:Removing node clone from graph, since it is a clone node which is the only user of placeholder arg1_1 and was inserted by the compiler.\n",
      "DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_input_alias_fixing_clones:Removed auxiliary clone nodes for placeholders:\n",
      "graph():\n",
      "    %arg0_1 : [num_users=1] = placeholder[target=arg0_1]\n",
      "    %arg1_1 : [num_users=1] = placeholder[target=arg1_1]\n",
      "    %relu : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%arg0_1,), kwargs = {})\n",
      "    %relu_1 : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%arg1_1,), kwargs = {})\n",
      "    %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%relu, %relu_1), kwargs = {})\n",
      "    %mean : [num_users=1] = call_function[target=torch.ops.aten.mean.default](args = (%add,), kwargs = {})\n",
      "    return (mean,)\n",
      "DEBUG:torch_tensorrt.dynamo.lowering.passes.constant_folding:Graph after constant folding:\n",
      "graph():\n",
      "    %arg0_1 : [num_users=1] = placeholder[target=arg0_1]\n",
      "    %arg1_1 : [num_users=1] = placeholder[target=arg1_1]\n",
      "    %relu : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%arg0_1,), kwargs = {})\n",
      "    %relu_1 : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%arg1_1,), kwargs = {})\n",
      "    %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%relu, %relu_1), kwargs = {})\n",
      "    %mean : [num_users=1] = call_function[target=torch.ops.aten.mean.default](args = (%add,), kwargs = {})\n",
      "    return (mean,)\n",
      "DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_assert_scalar:Removed 0 assert_scalar nodes:\n",
      "graph():\n",
      "    %arg0_1 : [num_users=1] = placeholder[target=arg0_1]\n",
      "    %arg1_1 : [num_users=1] = placeholder[target=arg1_1]\n",
      "    %relu : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%arg0_1,), kwargs = {})\n",
      "    %relu_1 : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%arg1_1,), kwargs = {})\n",
      "    %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%relu, %relu_1), kwargs = {})\n",
      "    %mean : [num_users=1] = call_function[target=torch.ops.aten.mean.default](args = (%add,), kwargs = {})\n",
      "    return (mean,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:torch_tensorrt.dynamo.lowering.passes.accumulate_fp32_matmul:Skipping FP32 accumulation for matmul layers as use_fp32_acc is not enabled in the compilation settings\n",
      "DEBUG:torch_tensorrt.dynamo.backend.backends:Lowered Input graph:\n",
      " graph():\n",
      "    %arg0_1 : [num_users=1] = placeholder[target=arg0_1]\n",
      "    %arg1_1 : [num_users=1] = placeholder[target=arg1_1]\n",
      "    %relu : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%arg0_1,), kwargs = {})\n",
      "    %relu_1 : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%arg1_1,), kwargs = {})\n",
      "    %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%relu, %relu_1), kwargs = {})\n",
      "    %mean : [num_users=1] = call_function[target=torch.ops.aten.mean.default](args = (%add,), kwargs = {})\n",
      "    return (mean,)\n",
      "DEBUG:torch_tensorrt.dynamo.conversion._ConverterRegistry:Converter options for aten.relu.default: 1\n",
      "DEBUG:torch_tensorrt.dynamo.conversion._ConverterRegistry:Selecting converter option 0 for converting aten.relu.default\n",
      "DEBUG:torch_tensorrt.dynamo.conversion._ConverterRegistry:Converter options for aten.relu.default: 1\n",
      "DEBUG:torch_tensorrt.dynamo.conversion._ConverterRegistry:Selecting converter option 0 for converting aten.relu.default\n",
      "DEBUG:torch_tensorrt.dynamo.conversion._ConverterRegistry:Converter options for aten.add.Tensor: 1\n",
      "DEBUG:torch_tensorrt.dynamo.conversion._ConverterRegistry:Selecting converter option 0 for converting aten.add.Tensor\n",
      "DEBUG:torch_tensorrt.dynamo.conversion._ConverterRegistry:Converter options for aten.mean.default: 1\n",
      "DEBUG:torch_tensorrt.dynamo.conversion._ConverterRegistry:Selecting converter option 0 for converting aten.mean.default\n",
      "DEBUG:torch_tensorrt.dynamo.partitioning._global_partitioner:\n",
      "Supported Nodes:\n",
      "- torch.ops.aten.relu.default + Operator Count: 2\n",
      "- torch.ops.aten.add.Tensor + Operator Count: 1\n",
      "- torch.ops.aten.mean.default + Operator Count: 1\n",
      "\n",
      "DEBUG:torch_tensorrt.dynamo.partitioning._global_partitioner:\n",
      "All Nodes Supported\n",
      "\n",
      "DEBUG:torch_tensorrt.dynamo._compiler:Detected support for 4 operators out of 4 in subgraph.\n",
      "INFO:torch_tensorrt.dynamo._compiler:Partitioning the graph via the fast partitioner\n",
      "DEBUG:torch_tensorrt.dynamo.conversion._ConverterRegistry:Converter options for aten.relu.default: 1\n",
      "DEBUG:torch_tensorrt.dynamo.conversion._ConverterRegistry:Selecting converter option 0 for converting aten.relu.default\n",
      "DEBUG:torch_tensorrt.dynamo.conversion._ConverterRegistry:Converter options for aten.relu.default: 1\n",
      "DEBUG:torch_tensorrt.dynamo.conversion._ConverterRegistry:Selecting converter option 0 for converting aten.relu.default\n",
      "DEBUG:torch_tensorrt.dynamo.conversion._ConverterRegistry:Converter options for aten.add.Tensor: 1\n",
      "DEBUG:torch_tensorrt.dynamo.conversion._ConverterRegistry:Selecting converter option 0 for converting aten.add.Tensor\n",
      "DEBUG:torch_tensorrt.dynamo.conversion._ConverterRegistry:Converter options for aten.mean.default: 1\n",
      "DEBUG:torch_tensorrt.dynamo.conversion._ConverterRegistry:Selecting converter option 0 for converting aten.mean.default\n",
      "DEBUG:torch_tensorrt.dynamo.partitioning._adjacency_partitioner:\n",
      "Number of TensorRT-Accelerated Engines Generated: 1\n",
      "DEBUG:torch_tensorrt.dynamo.partitioning._adjacency_partitioner:\n",
      "Supported Nodes:\n",
      "- torch.ops.aten.relu.default + Operator Count: 2\n",
      "- torch.ops.aten.add.Tensor + Operator Count: 1\n",
      "- torch.ops.aten.mean.default + Operator Count: 1\n",
      "\n",
      "DEBUG:torch_tensorrt.dynamo.partitioning._adjacency_partitioner:\n",
      "All Nodes Supported\n",
      "\n",
      "DEBUG:torch_tensorrt.dynamo._compiler:Updated metadata for node: _run_on_acc_0 with its corresponding submodule outputs\n",
      "DEBUG:torch_tensorrt.dynamo._compiler:Converting submodule: _run_on_acc_0\n",
      " Input shapes: [(5, 7), (5, 7)]\n",
      " graph():\n",
      "    %arg0_1 : [num_users=1] = placeholder[target=arg0_1]\n",
      "    %relu : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%arg0_1,), kwargs = {})\n",
      "    %arg1_1 : [num_users=1] = placeholder[target=arg1_1]\n",
      "    %relu_1 : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%arg1_1,), kwargs = {})\n",
      "    %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%relu, %relu_1), kwargs = {})\n",
      "    %mean : [num_users=1] = call_function[target=torch.ops.aten.mean.default](args = (%add,), kwargs = {})\n",
      "    return mean\n",
      "DEBUG:torch_tensorrt.dynamo.conversion._ConverterRegistry:Converter options for aten.relu.default: 1\n",
      "DEBUG:torch_tensorrt.dynamo.conversion._ConverterRegistry:Selecting converter option 0 for converting aten.relu.default\n",
      "DEBUG:torch_tensorrt.dynamo.conversion._ConverterRegistry:Converter options for aten.relu.default: 1\n",
      "DEBUG:torch_tensorrt.dynamo.conversion._ConverterRegistry:Selecting converter option 0 for converting aten.relu.default\n",
      "DEBUG:torch_tensorrt.dynamo.conversion._ConverterRegistry:Converter options for aten.add.Tensor: 1\n",
      "DEBUG:torch_tensorrt.dynamo.conversion._ConverterRegistry:Selecting converter option 0 for converting aten.add.Tensor\n",
      "DEBUG:torch_tensorrt.dynamo.conversion._ConverterRegistry:Converter options for aten.mean.default: 1\n",
      "DEBUG:torch_tensorrt.dynamo.conversion._ConverterRegistry:Selecting converter option 0 for converting aten.mean.default\n",
      "DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node arg0_1 (kind: arg0_1, args: ())\n",
      "DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Adding input to in-progress INetwork: arg0_1 [shape=[5, 7], dtype=DataType.HALF]\n",
      "INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converted node arg0_1 [arg0_1] (Inputs: () | Outputs: (arg0_1: (5, 7)@torch.float16))\n",
      "DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu (kind: aten.relu.default, args: ('arg0_1 <Node>',))\n",
      "DEBUG:torch_tensorrt.dynamo.conversion._ConverterRegistry:Converter options for aten.relu.default: 1\n",
      "DEBUG:torch_tensorrt.dynamo.conversion._ConverterRegistry:Selecting converter option 0 for converting aten.relu.default\n",
      "INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converted node relu [aten.relu.default] (Inputs: (arg0_1: (5, 7)@torch.float16) | Outputs: (relu: (5, 7)@torch.float16))\n",
      "DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node arg1_1 (kind: arg1_1, args: ())\n",
      "DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Adding input to in-progress INetwork: arg1_1 [shape=[5, 7], dtype=DataType.HALF]\n",
      "INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converted node arg1_1 [arg1_1] (Inputs: () | Outputs: (arg1_1: (5, 7)@torch.float16))\n",
      "DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_1 (kind: aten.relu.default, args: ('arg1_1 <Node>',))\n",
      "DEBUG:torch_tensorrt.dynamo.conversion._ConverterRegistry:Converter options for aten.relu.default: 1\n",
      "DEBUG:torch_tensorrt.dynamo.conversion._ConverterRegistry:Selecting converter option 0 for converting aten.relu.default\n",
      "INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converted node relu_1 [aten.relu.default] (Inputs: (arg1_1: (5, 7)@torch.float16) | Outputs: (relu_1: (5, 7)@torch.float16))\n",
      "DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node add (kind: aten.add.Tensor, args: ('relu <Node>', 'relu_1 <Node>'))\n",
      "DEBUG:torch_tensorrt.dynamo.conversion._ConverterRegistry:Converter options for aten.add.Tensor: 1\n",
      "DEBUG:torch_tensorrt.dynamo.conversion._ConverterRegistry:Selecting converter option 0 for converting aten.add.Tensor\n",
      "INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converted node add [aten.add.Tensor] (Inputs: (relu: (5, 7)@torch.float16, relu_1: (5, 7)@torch.float16) | Outputs: (add: (5, 7)@torch.float16))\n",
      "DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node mean (kind: aten.mean.default, args: ('add <Node>',))\n",
      "DEBUG:torch_tensorrt.dynamo.conversion._ConverterRegistry:Converter options for aten.mean.default: 1\n",
      "DEBUG:torch_tensorrt.dynamo.conversion._ConverterRegistry:Selecting converter option 0 for converting aten.mean.default\n",
      "INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converted node mean [aten.mean.default] (Inputs: (add: (5, 7)@torch.float16) | Outputs: (mean: ()@torch.float16))\n",
      "DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node output (kind: output, args: ('mean <Node>',))\n",
      "DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Marking output output0 [shape=(), dtype=DataType.HALF]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converted node output [output] (Inputs: (mean: ()@torch.float16) | Outputs: (output: ))\n",
      "INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:TRT INetwork construction elapsed time: 0:00:00.004228\n",
      "INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:Not found cached TRT engines. Start building engine.\n",
      "WARNING:py.warnings:/home/bryan/anaconda3/envs/mmcv/lib/python3.12/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter support\n",
      "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
      "\n",
      "INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:Using optimization level 4\n",
      "INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:Build TRT engine elapsed time: 0:00:01.153612\n",
      "INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:TRT Engine uses: 11996 bytes of Memory\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG: [Torch-TensorRT] - Deserializing Device Info: 0%8%9%0%NVIDIA GeForce RTX 4090\n",
      "DEBUG: [Torch-TensorRT] - Deserialized Device Info: Device(ID: 0, Name: NVIDIA GeForce RTX 4090, SM Capability: 8.9, Type: GPU)\n",
      "DEBUG: [Torch-TensorRT] - Target Device: Device(ID: 0, Name: NVIDIA GeForce RTX 4090, SM Capability: 8.9, Type: GPU)\n",
      "WARNING: [Torch-TensorRT] - Detected this engine is being instantitated in a multi-GPU system with multi-device safe mode disabled. For more on the implications of this as well as workarounds, see the linked documentation (https://pytorch.org/TensorRT/user_guide/runtime.html#multi-device-safe-mode)\n",
      "DEBUG: [Torch-TensorRT] - Setting Device(ID: 0, Name: NVIDIA GeForce RTX 4090, SM Capability: 8.9, Type: GPU) as active device\n",
      "INFO: [Torch-TensorRT] - Loaded engine size: 0 MiB\n",
      "DEBUG: [Torch-TensorRT] - Deserialization required 59 microseconds.\n",
      "DEBUG: [Torch-TensorRT] - Total per-runner device persistent memory is 0\n",
      "DEBUG: [Torch-TensorRT] - Total per-runner host persistent memory is 80\n",
      "DEBUG: [Torch-TensorRT] - Allocated device scratch memory of size 0\n",
      "DEBUG: [Torch-TensorRT] - - Runner scratch: 0 bytes\n",
      "INFO: [Torch-TensorRT] - [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)\n",
      "DEBUG: [Torch-TensorRT] - CUDA lazy loading is enabled.\n",
      "DEBUG: [Torch-TensorRT] - Input binding name: arg0_1 has TensorRT binding index: 0, Torch binding index: 0\n",
      "DEBUG: [Torch-TensorRT] - Input binding name: arg1_1 has TensorRT binding index: 1, Torch binding index: 1\n",
      "DEBUG: [Torch-TensorRT] - Output binding name: output0 has TensorRT binding index: 2, Torch binding index: 2\n",
      "DEBUG: [Torch-TensorRT] - Torch-TensorRT TensorRT Engine:\n",
      "  Name: _run_on_acc_0_engine\n",
      "  Inputs: [\n",
      "    id: 0\n",
      "      name: arg0_1\n",
      "      shape: [5, 7]\n",
      "      dtype: Half\n",
      "    id: 1\n",
      "      name: arg1_1\n",
      "      shape: [5, 7]\n",
      "      dtype: Half\n",
      "  ]\n",
      "  Outputs: [\n",
      "    id: 0\n",
      "      name: output0\n",
      "      shape: []\n",
      "      dtype: Half\n",
      "  ]\n",
      "  Device: Device(ID: 0, Name: NVIDIA GeForce RTX 4090, SM Capability: 8.9, Type: GPU)\n",
      "  Hardware Compatibility: Disabled\n",
      "  Target Platform: linux_x86_64\n",
      "\n",
      "DEBUG:torch_tensorrt.dynamo._DryRunTracker:\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++ Dry-Run Results for Graph ++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "The graph consists of 4 Total Operators, of which 4 operators are supported, 100.0% coverage\n",
      "\n",
      "Compiled with: CompilationSettings(enabled_precisions={<dtype.f16: 6>}, debug=True, workspace_size=0, min_block_size=2, torch_executed_ops={'torch.ops.aten.sub.Tensor'}, pass_through_build_failures=False, max_aux_streams=None, version_compatible=False, optimization_level=4, use_python_runtime=False, truncate_double=False, use_fast_partitioner=True, enable_experimental_decompositions=False, device=Device(type=DeviceType.GPU, gpu_id=0), require_full_compilation=False, disable_tf32=False, assume_dynamic_shape_support=False, sparse_weights=False, engine_capability=<EngineCapability.STANDARD: 1>, num_avg_timing_iters=1, dla_sram_size=1048576, dla_local_dram_size=1073741824, dla_global_dram_size=536870912, dryrun=False, hardware_compatible=False, timing_cache_path='/tmp/torch_tensorrt_engine_cache/timing_cache.bin', lazy_engine_init=False, cache_built_engines=False, reuse_cached_engines=False, use_explicit_typing=False, use_fp32_acc=False, refit_identical_engine_weights=False, strip_engine_weights=False, immutable_weights=True, enable_weight_streaming=False, enable_cross_compile_for_windows=False, use_aot_joint_export=True)\n",
      "\n",
      "  Graph Structure:\n",
      "\n",
      "   Inputs: List[Tensor: (5, 7)@float16, Tensor: (5, 7)@float16]\n",
      "    ...\n",
      "    TRT Engine #1 - Submodule name: _run_on_acc_0\n",
      "     Engine Inputs: List[Tensor: (5, 7)@float16, Tensor: (5, 7)@float16]\n",
      "     Number of Operators in Engine: 4\n",
      "     Engine Outputs: List[Tensor:)@float16]\n",
      "    ...\n",
      "   Outputs: List[Tensor:)@float16]\n",
      "\n",
      "  ------------------------- Aggregate Stats -------------------------\n",
      "\n",
      "   Average Number of Operators per TRT Engine: 4.0\n",
      "   Most Operators in a TRT Engine: 4\n",
      "\n",
      "  ********** Recommendations **********\n",
      "\n",
      "   - For minimal graph segmentation, select min_block_size=4 which would generate 1 TRT engine(s)\n",
      "   - The current level of graph segmentation is equivalent to selecting min_block_size=4 which generates 1 TRT engine(s)\n",
      "DEBUG: [Torch-TensorRT] - Attempting to run engine (ID: _run_on_acc_0_engine); Hardware Compatible: 0\n",
      "DEBUG: [Torch-TensorRT] - Input shape changed None -> (5,7)(5,7)\n",
      "DEBUG: [Torch-TensorRT] - Input Name: arg0_1 Shape: [5, 7]\n",
      "DEBUG: [Torch-TensorRT] - Input Name: arg1_1 Shape: [5, 7]\n",
      "DEBUG: [Torch-TensorRT] - Output Name: output0 Shape: []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.9287, device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we want to customize certain options in the backend,\n",
    "# but still use the torch.compile call directly, we can provide\n",
    "# custom options to the backend via the \"options\" keyword\n",
    "# which takes in a dictionary mapping options to values.\n",
    "#\n",
    "# For accepted backend options, see the CompilationSettings dataclass:\n",
    "# py/torch_tensorrt/dynamo/_settings.py\n",
    "backend_kwargs = {\n",
    "    \"enabled_precisions\": {torch.half},\n",
    "    \"debug\": True,\n",
    "    \"min_block_size\": 2,\n",
    "    \"torch_executed_ops\": {\"torch.ops.aten.sub.Tensor\"},\n",
    "    \"optimization_level\": 4,\n",
    "    \"use_python_runtime\": False,\n",
    "}\n",
    "\n",
    "# Run the model on an input to cause compilation, as so:\n",
    "optimized_model_custom = torch.compile(\n",
    "    model_half,\n",
    "    backend=\"torch_tensorrt\",\n",
    "    options=backend_kwargs,\n",
    "    dynamic=False,\n",
    ")\n",
    "optimized_model_custom(*sample_inputs_half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44003dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we use Torch utilities to clean up the workspace\n",
    "torch._dynamo.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b2a6eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mmcv]",
   "language": "python",
   "name": "conda-env-mmcv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
